{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_to_cons = pd.read_csv('process_data/all_ims_guide.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>im_lat</th>\n",
       "      <th>im_lon</th>\n",
       "      <th>clust_lat</th>\n",
       "      <th>clust_lon</th>\n",
       "      <th>nightlight</th>\n",
       "      <th>consumption</th>\n",
       "      <th>nightlight_bin</th>\n",
       "      <th>images</th>\n",
       "      <th>clust_num</th>\n",
       "      <th>images_renamed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.125000</td>\n",
       "      <td>35.174999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.125_35.174999.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.125_35.174999_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.133333</td>\n",
       "      <td>35.174999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.133333_35.174999.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.133333_35.174999_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.066666</td>\n",
       "      <td>35.191666</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.066666_35.191666.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.066666_35.191666_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-17.050000</td>\n",
       "      <td>35.199999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.05_35.199999.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.05_35.199999_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.100000</td>\n",
       "      <td>35.199999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.1_35.199999.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.1_35.199999_0.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      im_lat     im_lon  clust_lat  clust_lon  nightlight  consumption  \\\n",
       "0 -17.125000  35.174999  -17.09515  35.217213         0.0     2.039307   \n",
       "1 -17.133333  35.174999  -17.09515  35.217213         0.0     2.039307   \n",
       "2 -17.066666  35.191666  -17.09515  35.217213         0.0     2.039307   \n",
       "3 -17.050000  35.199999  -17.09515  35.217213         0.0     2.039307   \n",
       "4 -17.100000  35.199999  -17.09515  35.217213         0.0     2.039307   \n",
       "\n",
       "   nightlight_bin                    images  clust_num  \\\n",
       "0               1     -17.125_35.174999.png          0   \n",
       "1               1  -17.133333_35.174999.png          0   \n",
       "2               1  -17.066666_35.191666.png          0   \n",
       "3               1      -17.05_35.199999.png          0   \n",
       "4               1       -17.1_35.199999.png          0   \n",
       "\n",
       "               images_renamed  \n",
       "0     -17.125_35.174999_0.png  \n",
       "1  -17.133333_35.174999_0.png  \n",
       "2  -17.066666_35.191666_0.png  \n",
       "3      -17.05_35.199999_0.png  \n",
       "4       -17.1_35.199999_0.png  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_to_cons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the data into a PyTorch Tensor\n",
    "data_transforms = {\n",
    "    'transform': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this if you want to see how many the model gets right on the train set. Keep in mind that nightlights is not the main objective but just a means to an end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ft.eval()   # Set model to evaluate mode\n",
    "\n",
    "# running_loss = 0.0\n",
    "# running_corrects = 0\n",
    "# total = 0\n",
    "\n",
    "# # Iterate over data.\n",
    "# for inputs, labels in dataloaders_dict['train']:\n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = labels.to(device)\n",
    "\n",
    "#     # forward\n",
    "#     # track history if only in train\n",
    "#     with torch.set_grad_enabled(False):\n",
    "#         outputs = model_ft(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         _, preds = torch.max(outputs, 1)\n",
    "\n",
    "#     # statistics\n",
    "#     running_loss += loss.item() * inputs.size(0)\n",
    "#     running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "#     total += len(preds)\n",
    "#     if total == 1000:\n",
    "#         break\n",
    "        \n",
    "# running_corrects.double()/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def filename_to_im_tensor(file):\n",
    "    im = plt.imread(file)[:,:,:3]\n",
    "    im = data_transforms['transform'](im)\n",
    "    return im[None].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = torch.load('trained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Dropout(p=0.5)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (4): ReLU(inplace)\n",
       "  (5): Dropout(p=0.5)\n",
       "  (6): Linear(in_features=4096, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Dropout(p=0.5)\n",
       "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we \"rip\" off the final layers\n",
    "ripped = model_ft.classifier\n",
    "del ripped[6]\n",
    "del ripped[5]\n",
    "del ripped[4]\n",
    "ripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU(inplace)\n",
       "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU(inplace)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU(inplace)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): ReLU(inplace)\n",
       "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (24): ReLU(inplace)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (27): ReLU(inplace)\n",
       "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19997, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_ims = im_to_cons.drop_duplicates(subset='images'); unique_ims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.zeros((unique_ims.shape[0],4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU(inplace)\n",
       "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU(inplace)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU(inplace)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): ReLU(inplace)\n",
       "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (24): ReLU(inplace)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (27): ReLU(inplace)\n",
       "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, 10000, 10100, 10200, 10300, 10400, 10500, 10600, 10700, 10800, 10900, 11000, 11100, 11200, 11300, 11400, 11500, 11600, 11700, 11800, 11900, 12000, 12100, 12200, 12300, 12400, 12500, 12600, 12700, 12800, 12900, 13000, 13100, 13200, 13300, 13400, 13500, 13600, 13700, 13800, 13900, 14000, 14100, 14200, 14300, 14400, 14500, 14600, 14700, 14800, 14900, 15000, 15100, 15200, 15300, 15400, 15500, 15600, 15700, 15800, 15900, 16000, 16100, 16200, 16300, 16400, 16500, 16600, 16700, 16800, 16900, 17000, 17100, 17200, 17300, 17400, 17500, 17600, 17700, 17800, 17900, 18000, 18100, 18200, 18300, 18400, 18500, 18600, 18700, 18800, 18900, 19000, 19100, 19200, 19300, 19400, 19500, 19600, 19700, 19800, 19900, "
     ]
    }
   ],
   "source": [
    "# this might take around 45 minutes\n",
    "model_ft.eval()\n",
    "for i, im_name in enumerate(unique_ims['images']):\n",
    "    image = filename_to_im_tensor('process_data/data/ims_malawi_2016/{}'.format(im_name))\n",
    "    res = np.array(model_ft(image).cpu().detach().numpy().tolist()[0])\n",
    "    feats[i,:] += res\n",
    "    if i % 100 == 0:\n",
    "        print(i, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save them for safekeeping\n",
    "np.save('forward_feats_trained_model.npy', feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>im_lat</th>\n",
       "      <th>im_lon</th>\n",
       "      <th>clust_lat</th>\n",
       "      <th>clust_lon</th>\n",
       "      <th>nightlight</th>\n",
       "      <th>consumption</th>\n",
       "      <th>nightlight_bin</th>\n",
       "      <th>images</th>\n",
       "      <th>clust_num</th>\n",
       "      <th>images_renamed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.125000</td>\n",
       "      <td>35.174999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.125_35.174999.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.125_35.174999_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.133333</td>\n",
       "      <td>35.174999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.133333_35.174999.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.133333_35.174999_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.066666</td>\n",
       "      <td>35.191666</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.066666_35.191666.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.066666_35.191666_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-17.050000</td>\n",
       "      <td>35.199999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.05_35.199999.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.05_35.199999_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.100000</td>\n",
       "      <td>35.199999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.1_35.199999.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.1_35.199999_0.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      im_lat     im_lon  clust_lat  clust_lon  nightlight  consumption  \\\n",
       "0 -17.125000  35.174999  -17.09515  35.217213         0.0     2.039307   \n",
       "1 -17.133333  35.174999  -17.09515  35.217213         0.0     2.039307   \n",
       "2 -17.066666  35.191666  -17.09515  35.217213         0.0     2.039307   \n",
       "3 -17.050000  35.199999  -17.09515  35.217213         0.0     2.039307   \n",
       "4 -17.100000  35.199999  -17.09515  35.217213         0.0     2.039307   \n",
       "\n",
       "   nightlight_bin                    images  clust_num  \\\n",
       "0               1     -17.125_35.174999.png          0   \n",
       "1               1  -17.133333_35.174999.png          0   \n",
       "2               1  -17.066666_35.191666.png          0   \n",
       "3               1      -17.05_35.199999.png          0   \n",
       "4               1       -17.1_35.199999.png          0   \n",
       "\n",
       "               images_renamed  \n",
       "0     -17.125_35.174999_0.png  \n",
       "1  -17.133333_35.174999_0.png  \n",
       "2  -17.066666_35.191666_0.png  \n",
       "3      -17.05_35.199999_0.png  \n",
       "4       -17.1_35.199999_0.png  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_ims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ims = unique_ims[['images']]\n",
    "unique_ims['feat_index'] = np.arange(len(unique_ims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>feat_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.125_35.174999.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.133333_35.174999.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.066666_35.191666.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-17.05_35.199999.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.1_35.199999.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     images  feat_index\n",
       "0     -17.125_35.174999.png           0\n",
       "1  -17.133333_35.174999.png           1\n",
       "2  -17.066666_35.191666.png           2\n",
       "3      -17.05_35.199999.png           3\n",
       "4       -17.1_35.199999.png           4"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_ims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_to_cons = pd.merge(left=im_to_cons, right=unique_ims, on='images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>im_lat</th>\n",
       "      <th>im_lon</th>\n",
       "      <th>clust_lat</th>\n",
       "      <th>clust_lon</th>\n",
       "      <th>nightlight</th>\n",
       "      <th>consumption</th>\n",
       "      <th>nightlight_bin</th>\n",
       "      <th>images</th>\n",
       "      <th>clust_num</th>\n",
       "      <th>images_renamed</th>\n",
       "      <th>feat_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.125000</td>\n",
       "      <td>35.174999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.125_35.174999.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.125_35.174999_0.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.133333</td>\n",
       "      <td>35.174999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.133333_35.174999.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.133333_35.174999_0.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.066666</td>\n",
       "      <td>35.191666</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.066666_35.191666.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.066666_35.191666_0.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-17.050000</td>\n",
       "      <td>35.199999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.05_35.199999.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.05_35.199999_0.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.100000</td>\n",
       "      <td>35.199999</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.039307</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.1_35.199999.png</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.1_35.199999_0.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      im_lat     im_lon  clust_lat  clust_lon  nightlight  consumption  \\\n",
       "0 -17.125000  35.174999  -17.09515  35.217213         0.0     2.039307   \n",
       "1 -17.133333  35.174999  -17.09515  35.217213         0.0     2.039307   \n",
       "2 -17.066666  35.191666  -17.09515  35.217213         0.0     2.039307   \n",
       "3 -17.050000  35.199999  -17.09515  35.217213         0.0     2.039307   \n",
       "4 -17.100000  35.199999  -17.09515  35.217213         0.0     2.039307   \n",
       "\n",
       "   nightlight_bin                    images  clust_num  \\\n",
       "0               1     -17.125_35.174999.png          0   \n",
       "1               1  -17.133333_35.174999.png          0   \n",
       "2               1  -17.066666_35.191666.png          0   \n",
       "3               1      -17.05_35.199999.png          0   \n",
       "4               1       -17.1_35.199999.png          0   \n",
       "\n",
       "               images_renamed  feat_index  \n",
       "0     -17.125_35.174999_0.png           0  \n",
       "1  -17.133333_35.174999_0.png           1  \n",
       "2  -17.066666_35.191666_0.png           2  \n",
       "3      -17.05_35.199999_0.png           3  \n",
       "4       -17.1_35.199999_0.png           4  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_to_cons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = im_to_cons.groupby(['clust_lat', 'clust_lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "778"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusts = len(group); num_clusts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((num_clusts, 4096))\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this goes through each cluster group and finds all images that are in the cluster\n",
    "# it aggregates the features for those images across the cluster\n",
    "for i, g in enumerate(group):\n",
    "    lat, long = g[0]\n",
    "    im_sub = im_to_cons[(im_to_cons['clust_lat'] == lat) & (im_to_cons['clust_lon'] == long)].reset_index(drop=True)\n",
    "    agg_feats = np.zeros((len(im_sub), 4096))\n",
    "    for j, d in im_sub.iterrows():\n",
    "        agg_feats[j,:] = feats[d.feat_index]\n",
    "    agg_feats = agg_feats.mean(axis=0) # averages the features across all images in the cluster\n",
    "    \n",
    "    x[i,:] = agg_feats\n",
    "    y.append(g[1]['consumption'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "y_log = np.log(y) # try predicting consumption and log consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a bunch of code from the Jean et al Github that is modified to work with Python3 and our data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.linear_model as linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import EllipseCollection\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def predict_consumption(\n",
    "    X, y, dimension=None, k=5, k_inner=5, points=10,\n",
    "        alpha_low=1, alpha_high=5, margin=0.25):\n",
    "    \"\"\"\n",
    "    Plots predicted consumption\n",
    "    \"\"\"\n",
    "    X = reduce_dimension(X, dimension)\n",
    "    y_hat, r2 = run_cv(X, y, k, k_inner, points, alpha_low, alpha_high)\n",
    "    return X, y, y_hat, r2\n",
    "\n",
    "\n",
    "def plot_predictions(country, y, y_hat, r2, margin):\n",
    "    \"\"\"\n",
    "    Plots consumption predictions vs. true values.\n",
    "    \"\"\"\n",
    "    slope, intercept, ymin, ymax, xmin, xmax = compute_plot_params(\n",
    "        y, y_hat, margin)\n",
    "    sns.set_style('white')\n",
    "    plt.figure()\n",
    "    plt.axis('equal')\n",
    "    plt.scatter(y, y_hat, edgecolor='k', color='lightblue', s=20, marker='o')\n",
    "    x_trend = np.array([xmin, xmax]) * 1.5\n",
    "    y_trend = slope * x_trend + intercept\n",
    "    plt.plot(x_trend, y_trend, 'b-', linewidth=2,\n",
    "             color=sns.xkcd_rgb['french blue'])\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(ymin, ymax)\n",
    "    plt.xlabel('Log consumption expenditures', fontsize=14)\n",
    "    plt.ylabel('Model predictions', fontsize=14)\n",
    "    plt.title(country.capitalize() + ': $r^2 = {0:.2f}$'.format(r2))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_plot_params(y, y_hat, margin):\n",
    "    \"\"\"\n",
    "    Computes parameters for plotting consumption predictions vs. true values.\n",
    "    \"\"\"\n",
    "    slope, intercept, _, _, _ = stats.linregress(y, y_hat)\n",
    "    ymin = min(y_hat) - margin\n",
    "    ymax = max(y_hat) + margin\n",
    "    xmin = min(y) - margin\n",
    "    xmax = max(y) + margin\n",
    "    return slope, intercept, ymin, ymax, xmin, xmax\n",
    "\n",
    "\n",
    "def compare_models(\n",
    "    country_path, survey, percentiles, dimension, k, k_inner, trials,\n",
    "        poverty_line, multiples):\n",
    "    \"\"\"\n",
    "    Evaluates and plots comparison of transfer learning and nightlight models.\n",
    "    \"\"\"\n",
    "    r2s, r2s_nl = evaluate_percentiles(\n",
    "        country_path, survey, percentiles, dimension, k, k_inner, trials)\n",
    "    if survey == 'lsms':\n",
    "        X, X_nl, y = load_and_reduce_country_by_percentile(\n",
    "            country_path, survey, 1.0, dimension)\n",
    "        fractions = compute_fractions(poverty_line, multiples, y)\n",
    "        plot_percentiles_lsms(percentiles, multiples, r2s, r2s_nl, fractions)\n",
    "    elif survey == 'dhs':\n",
    "        plot_percentiles_dhs(percentiles, r2s, r2s_nl)\n",
    "\n",
    "\n",
    "def load_and_reduce_country_by_percentile(\n",
    "        country_path, survey, percentile, dimension):\n",
    "    \"\"\"\n",
    "    Loads data for one country up to a certain percentile.\n",
    "    \"\"\"\n",
    "    if survey == 'lsms':\n",
    "        X, X_nl, y = load_country_lsms(country_path)\n",
    "    elif survey == 'dhs':\n",
    "        X, X_nl, y = load_country_dhs(country_path)\n",
    "    X, X_nl, y = threshold_by_percentile(X, X_nl, y, percentile)\n",
    "    X = reduce_dimension(X, dimension)\n",
    "    return X, X_nl, y\n",
    "\n",
    "\n",
    "def threshold_by_percentile(X, X_nl, y, percentile):\n",
    "    \"\"\"\n",
    "    Threshold data by output percentile.\n",
    "    \"\"\"\n",
    "    threshold = np.percentile(y, q=100*percentile)\n",
    "    X = X[y <= threshold]\n",
    "    X_nl = X_nl[y <= threshold]\n",
    "    y = y[y <= threshold]\n",
    "    return X, X_nl, y\n",
    "\n",
    "\n",
    "def evaluate_percentiles(\n",
    "        country_path, survey, percentiles, dimension, k, k_inner, trials):\n",
    "    \"\"\"\n",
    "    Evaluate transfer learning and nightlight models for each percentile.\n",
    "    \"\"\"\n",
    "    r2s = np.zeros((len(percentiles), trials))\n",
    "    r2s_nl = np.zeros((len(percentiles), trials))\n",
    "    for idx, percentile in enumerate(percentiles):\n",
    "        for trial in xrange(trials):\n",
    "            X, X_nl, y = load_and_reduce_country_by_percentile(\n",
    "                country_path, survey, percentile, dimension)\n",
    "            _, r2 = run_cv(\n",
    "                X, y, k, k_inner, points=10, alpha_low=0, alpha_high=3,\n",
    "                randomize=False)\n",
    "            r2_nl = run_cv_ols(X_nl, y, k)\n",
    "            r2s[idx, trial] = r2\n",
    "            r2s_nl[idx, trial] = r2_nl\n",
    "    r2s = r2s.mean(axis=1)\n",
    "    r2s_nl = r2s_nl.mean(axis=1)\n",
    "    return r2s, r2s_nl\n",
    "\n",
    "\n",
    "def run_cv_ols(X, y, k):\n",
    "    \"\"\"\n",
    "    Runs OLS in cross-validation to compute r-squared.\n",
    "    \"\"\"\n",
    "    r2s = np.zeros((k,))\n",
    "    kf = cross_validation.KFold(n=y.size, n_folds=k, shuffle=True)\n",
    "    fold = 0\n",
    "    for train_idx, test_idx in kf:\n",
    "        r2s, fold = evaluate_fold_ols(X, y, train_idx, test_idx, r2s, fold)\n",
    "    return r2s.mean()\n",
    "\n",
    "\n",
    "def evaluate_fold_ols(X, y, train_idx, test_idx, r2s, fold):\n",
    "    \"\"\"\n",
    "    Evaluates one fold of outer CV using OLS.\n",
    "    \"\"\"\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    X_train, X_test = scale_features(X_train, X_test)\n",
    "    y_test_hat = train_and_predict_ols(X_train, y_train, X_test)\n",
    "    r2 = stats.pearsonr(y_test, y_test_hat)[0] ** 2\n",
    "    if np.isnan(r2):\n",
    "        r2 = 0\n",
    "    r2s[fold] = r2\n",
    "    return r2s, fold + 1\n",
    "\n",
    "\n",
    "def train_and_predict_ols(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Trains OLS model and predicts test set.\n",
    "    \"\"\"\n",
    "    ols = linear_model.LinearRegression()\n",
    "    ols.fit(X_train, y_train)\n",
    "    y_hat = ols.predict(X_test)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def compute_fractions(poverty_line, multiples, y):\n",
    "    \"\"\"\n",
    "    Computes the fraction of clusters below each multiple of the poverty line.\n",
    "    \"\"\"\n",
    "    fractions = np.zeros((len(multiples),))\n",
    "    for idx, multiple in enumerate(multiples):\n",
    "        fractions[idx] = (\n",
    "            np.exp(y) <= poverty_line * multiple).sum() / float(y.size)\n",
    "    return fractions\n",
    "\n",
    "\n",
    "def plot_percentiles_lsms(percentiles, multiples, r2s, r2s_nl, fractions):\n",
    "    \"\"\"\n",
    "    Plots transfer learning model vs. nightlights model at each percentile.\n",
    "    \"\"\"\n",
    "    sns.set_style('white')\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    lines = []\n",
    "    percentiles = [100 * x for x in percentiles]\n",
    "    for idx, multiple in enumerate(multiples):\n",
    "        lines.append(\n",
    "            plt.axvline(\n",
    "                100 * fractions[idx], color='r', linestyle='dashed',\n",
    "                linewidth=3.0 / (idx + 1),\n",
    "                label=str(multiple) + 'x poverty line'))\n",
    "    line_legend = plt.legend(\n",
    "        handles=lines, title='Poverty line multiples:', loc='upper right',\n",
    "        bbox_to_anchor=(0.5, 1), fontsize=10)\n",
    "    plt.gca().add_artist(line_legend)\n",
    "    curve1, = plt.plot(percentiles, r2s, label='Transfer learning')\n",
    "    curve2, = plt.plot(percentiles, r2s_nl, label='Nightlights')\n",
    "    plt.legend(\n",
    "        handles=[curve1, curve2], loc='upper right',\n",
    "        bbox_to_anchor=(0.5, 0.65))\n",
    "    plt.xlabel('Poorest percent of clusters used', fontsize=14)\n",
    "    plt.ylabel('$r^2$', fontsize=18)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_percentiles_dhs(percentiles, r2s, r2s_nl):\n",
    "    \"\"\"\n",
    "    Plots transfer learning model vs. nightlights model at each\n",
    "    \"\"\"\n",
    "    sns.set_style('white')\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    percentiles = [100 * x for x in percentiles]\n",
    "    plt.plot(percentiles, r2s)\n",
    "    plt.plot(percentiles, r2s_nl)\n",
    "    plt.legend(['Transfer learning', 'Nightlights'], loc='upper center')\n",
    "    plt.xlabel('Poorest percent of clusters used', fontsize=14)\n",
    "    plt.ylabel('$r^2$', fontsize=18)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_randomization_test(\n",
    "    country_names, country_paths, survey, dimension, k, k_inner, points,\n",
    "        alpha_low, alpha_high, trials):\n",
    "    \"\"\"\n",
    "    Runs randomization test for a set of countries.\n",
    "    \"\"\"\n",
    "    data = load_data(country_paths, survey, dimension)\n",
    "    true_r2s = compute_true_r2s(\n",
    "        data, k, k_inner, points, alpha_low, alpha_high)\n",
    "    shuffled_r2s = compute_shuffled_r2s(\n",
    "        data, k, k_inner, points, alpha_low, alpha_high, trials)\n",
    "    plot_shuffled_distributions(country_names, survey, shuffled_r2s, true_r2s)\n",
    "\n",
    "\n",
    "def compute_true_r2s(data, k, k_inner, points, alpha_low, alpha_high):\n",
    "    \"\"\"\n",
    "    Uses data to compute true model r2s.\n",
    "    \"\"\"\n",
    "    true_r2s = []\n",
    "    for (X, y) in data:\n",
    "        _, r2 = run_cv(X, y, k, k_inner, points, alpha_low, alpha_high)\n",
    "        true_r2s.append(r2)\n",
    "    return true_r2s\n",
    "\n",
    "\n",
    "def compute_shuffled_r2s(\n",
    "        data, k, k_inner, points, alpha_low, alpha_high, trials):\n",
    "    \"\"\"\n",
    "    Uses data to compute shuffled model r2s.\n",
    "    \"\"\"\n",
    "    shuffled_r2s = np.zeros((len(data), trials))\n",
    "    for data_idx, (X, y) in enumerate(data):\n",
    "        for trial in range(trials):\n",
    "            _, shuffled_r2s[data_idx, trial] = run_cv(\n",
    "                X, y, k, k_inner, points, alpha_low, alpha_high,\n",
    "                randomize=True)\n",
    "    return shuffled_r2s\n",
    "\n",
    "\n",
    "def plot_shuffled_distributions(country_names, survey, shuffled_r2s, true_r2s):\n",
    "    \"\"\"\n",
    "    Plots shuffled r2 distributions vs. true model r2s.\n",
    "    \"\"\"\n",
    "    colors = sns.color_palette('husl', len(country_names))\n",
    "    sns.set_style('white')\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    frame = plt.gca()\n",
    "    for i in xrange(len(country_names)):\n",
    "        plt.axvline(\n",
    "            true_r2s[i], color=colors[i], linestyle='dashed', linewidth=2,\n",
    "            label=country_names[i].capitalize() + ': $r^2={0:.2f}$'.format(\n",
    "                true_r2s[i]))\n",
    "    plt.legend(\n",
    "        title='True {} Models:'.format(survey.upper()), loc='lower right',\n",
    "        bbox_to_anchor=(0.6, 0.45), fontsize=12)\n",
    "    for i in xrange(len(country_names)):\n",
    "        sns.kdeplot(shuffled_r2s[i, :], shade=True, color=colors[i])\n",
    "    plt.xlim((0, max(true_r2s) + 0.05))\n",
    "    plt.xlabel('$r^2$', fontsize=18)\n",
    "    plt.ylabel('Randomized $r^2$ distribution', fontsize=14)\n",
    "    frame.yaxis.set_ticklabels([])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_models(\n",
    "    country_names, country_paths, survey, dimension, k, trials, points,\n",
    "        alpha_low, alpha_high, cmap='Greens'):\n",
    "    \"\"\"\n",
    "    Evaluates in- and out-of-country performance for models trained on set of\n",
    "    countries given.\n",
    "    \"\"\"\n",
    "    n = len(country_names)\n",
    "    performance_matrix = np.zeros((trials, n, n))\n",
    "    data = load_data(country_paths, survey, dimension=None)\n",
    "    for trial in xrange(trials):\n",
    "        for in_idx in xrange(n):\n",
    "            performance_matrix[trial, :, in_idx] = compute_column(\n",
    "                country_names[in_idx], data, in_idx, dimension, k, points,\n",
    "                alpha_low, alpha_high)\n",
    "    performance_matrix = performance_matrix.mean(axis=0)\n",
    "    plot_model_performance(performance_matrix, country_names, cmap)\n",
    "    return np.around(performance_matrix, decimals=2)\n",
    "\n",
    "\n",
    "def compute_column(\n",
    "    country_name, data, in_idx, dimension, k, points, alpha_low,\n",
    "        alpha_high):\n",
    "    \"\"\"\n",
    "    Evaluates in- and out-of-country performance for model trained on one\n",
    "    country.\n",
    "    \"\"\"\n",
    "    in_data = data[in_idx]\n",
    "    out_data = list(data)\n",
    "    out_data.pop(in_idx)\n",
    "    if country_name == 'pooled':\n",
    "        r2s_in, r2s_out = evaluate_pooled_r2s(\n",
    "            out_data, dimension, k, points, alpha_low, alpha_high)\n",
    "    else:\n",
    "        r2s_in, r2s_out = evaluate_model_r2s(\n",
    "            in_data, out_data, dimension, k, points, alpha_low, alpha_high)\n",
    "    max_idx = np.argmax(r2s_in)\n",
    "    r2s = list(r2s_out[:, max_idx])\n",
    "    r2s.insert(in_idx, r2s_in[max_idx])\n",
    "    r2s.reverse()\n",
    "    r2s = np.array(r2s)\n",
    "    return r2s\n",
    "\n",
    "\n",
    "def evaluate_pooled_r2s(all_data, dimension, k, points, alpha_low, alpha_high):\n",
    "    \"\"\"\n",
    "    Evaluates in- and out-of-country r2s for the pooled model.\n",
    "    \"\"\"\n",
    "    X_all_full, y_all = [list(data) for data in zip(*all_data)]\n",
    "    alphas = np.logspace(alpha_low, alpha_high, points)\n",
    "    r2s_in = np.zeros((k, points))\n",
    "    r2s_out = np.zeros((len(all_data), k, points))\n",
    "    kf = []\n",
    "    for i in xrange(len(X_all_full)):\n",
    "        kf.append(list(cross_validation.KFold(\n",
    "            n=all_data[i][1].size, n_folds=k, shuffle=True)))\n",
    "    for fold in xrange(k):\n",
    "        X_train, X_test, y_train, y_test = split_pooled_data(\n",
    "            kf, fold, X_all_full, y_all)\n",
    "        X_train, X_test, X_all = reduce_and_scale_features(\n",
    "            X_train, X_test, X_all_full, dimension)\n",
    "        r2s_in, r2s_out = evaluate_alphas(\n",
    "            X_train, X_test, X_all, y_train, y_test, y_all, r2s_in, r2s_out,\n",
    "            fold, alphas)\n",
    "    return r2s_in.mean(axis=0), r2s_out.mean(axis=1)\n",
    "\n",
    "\n",
    "def split_pooled_data(kf, fold, X_all_full, y_all):\n",
    "    \"\"\"\n",
    "    Splits pooled training and test data for each fold.\n",
    "    \"\"\"\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    for i in xrange(len(kf)):\n",
    "        train_ind = kf[i][fold][0]\n",
    "        test_ind = kf[i][fold][1]\n",
    "        X_train.append(X_all_full[i][train_ind])\n",
    "        X_test.append(X_all_full[i][test_ind])\n",
    "        y_train.append(y_all[i][train_ind])\n",
    "        y_test.append(y_all[i][test_ind])\n",
    "    X_train = np.vstack(X_train)\n",
    "    X_test = np.vstack(X_test)\n",
    "    y_train = np.hstack(y_train)\n",
    "    y_test = np.hstack(y_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def evaluate_model_r2s(\n",
    "        in_data, out_data, dimension, k, points, alpha_low, alpha_high):\n",
    "    \"\"\"\n",
    "    Evaluates in- and out-of-country r2s.\n",
    "    \"\"\"\n",
    "    X, y = in_data\n",
    "    X_out_full, y_out = [list(data) for data in zip(*out_data)]\n",
    "    alphas = np.logspace(alpha_low, alpha_high, points)\n",
    "    r2s_in = np.zeros((k, points))\n",
    "    r2s_out = np.zeros((len(out_data), k, points))\n",
    "    kf = cross_validation.KFold(n=in_data[1].size, n_folds=k, shuffle=True)\n",
    "    fold = 0\n",
    "    for train_ind, test_ind in kf:\n",
    "        X_train, X_test = X[train_ind], X[test_ind]\n",
    "        y_train, y_test = y[train_ind], y[test_ind]\n",
    "        X_train, X_test, X_out = reduce_and_scale_features(\n",
    "            X_train, X_test, X_out_full, dimension)\n",
    "        r2s_in, r2s_out = evaluate_alphas(\n",
    "            X_train, X_test, X_out, y_train, y_test, y_out, r2s_in, r2s_out,\n",
    "            fold, alphas)\n",
    "        fold += 1\n",
    "    return r2s_in.mean(axis=0), r2s_out.mean(axis=1)\n",
    "\n",
    "\n",
    "def evaluate_alphas(\n",
    "    X_train, X_test, X_out, y_train, y_test, y_out, r2s_in, r2s_out, fold,\n",
    "        alphas):\n",
    "    \"\"\"\n",
    "    Computes r2 for different regularization constants.\n",
    "    \"\"\"\n",
    "    for idx, alpha in enumerate(alphas):\n",
    "        ridge = linear_model.Ridge(alpha=alpha)\n",
    "        ridge.fit(X_train.astype(np.float), y_train.astype(np.float))\n",
    "        r2s_in[fold, idx] = compute_r2(ridge, X_test, y_test)\n",
    "        for i in xrange(len(X_out)):\n",
    "            r2s_out[i, fold, idx] = compute_r2(ridge, X_out[i], y_out[i])\n",
    "    return r2s_in, r2s_out\n",
    "\n",
    "\n",
    "def reduce_and_scale_features(X_train, X_test, X_out_full, dimension):\n",
    "    \"\"\"\n",
    "    Reduces dimension and scales in- and out-of-country features.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=dimension)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=False)\n",
    "    X_train = scaler.fit_transform(pca.fit_transform(X_train))\n",
    "    X_test = scaler.transform(pca.transform(X_test))\n",
    "    X_out = []\n",
    "    for X in X_out_full:\n",
    "        X_out.append(scaler.transform(pca.transform(X)))\n",
    "    return X_train, X_test, X_out\n",
    "\n",
    "\n",
    "def compute_r2(model, X, y):\n",
    "    \"\"\"\n",
    "    Computes model r2.\n",
    "    \"\"\"\n",
    "    y_hat = model.predict(X)\n",
    "    r2 = stats.pearsonr(y, y_hat)[0] ** 2\n",
    "    return r2\n",
    "\n",
    "\n",
    "def plot_circles(data, ax, **kwargs):\n",
    "    \"\"\"\n",
    "    Plots circles for r2 values.\n",
    "    \"\"\"\n",
    "    M = np.array(data)\n",
    "    xy = np.indices(M.shape)[::-1].reshape(2, -1).T\n",
    "    w = np.abs(M).ravel()\n",
    "    h = np.abs(M).ravel()\n",
    "    a = 0\n",
    "    circles = EllipseCollection(\n",
    "        widths=w, heights=h, angles=a, units='x', offsets=xy,\n",
    "        transOffset=ax.transData, array=M.ravel(), **kwargs)\n",
    "    ax.add_collection(circles)\n",
    "    ax.set_xticks(np.arange(M.shape[1]))\n",
    "    ax.set_xticklabels(data.columns)\n",
    "    ax.set_yticks(np.arange(M.shape[0]))\n",
    "    ax.set_yticklabels(data.index, rotation=90)\n",
    "    return circles\n",
    "\n",
    "\n",
    "def plot_model_performance(data, country_names, cmap='Greens'):\n",
    "    \"\"\"\n",
    "    Makes plot for in- and out-of-country model performance.\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame(np.flipud(data))\n",
    "    data.columns = [country.capitalize() for country in country_names]\n",
    "    data.index = [country.capitalize() for country in country_names]\n",
    "    sns.set_style('white')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    circles = plot_circles(data, ax=ax, cmap=cmap)\n",
    "    cb = fig.colorbar(circles)\n",
    "    cb.set_label('$r^2$', fontsize=16)\n",
    "    ax.margins(0.1)\n",
    "    plt.xlabel('Country trained on', fontsize=16)\n",
    "    plt.ylabel('Country evaluated on', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_data(country_paths, survey, dimension):\n",
    "    \"\"\"\n",
    "    Loads data for all surveys.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for country_path in country_paths:\n",
    "        X, y = load_and_reduce_country(country_path, survey, dimension)\n",
    "        data.append((X, y))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_country_lsms(country_path):\n",
    "    \"\"\"\n",
    "    Loads data for one LSMS country.\n",
    "    \"\"\"\n",
    "    X = np.load(country_path + 'cluster_conv_features.npy')\n",
    "    X_nl = np.load(country_path + 'cluster_nightlights.npy').reshape(-1, 1)\n",
    "    y = np.load(country_path + 'cluster_consumptions.npy')\n",
    "    hhs = np.load(country_path + 'cluster_households.npy')\n",
    "    images = np.load(country_path + 'cluster_image_counts.npy')\n",
    "    # Filter out single households and <10 images\n",
    "    mask = np.logical_and((hhs >= 2), (images >= 10))\n",
    "    X = X[mask]\n",
    "    X_nl = X_nl[mask]\n",
    "    y = y[mask]\n",
    "    # Filter out 0 consumption clusters\n",
    "    X = X[y > 0]\n",
    "    X_nl = X_nl[y > 0]\n",
    "    y = y[y > 0]\n",
    "    y = np.log(y)\n",
    "    return X, X_nl, y\n",
    "\n",
    "\n",
    "def load_country_dhs(country_path):\n",
    "    \"\"\"\n",
    "    Loads data for one DHS country.\n",
    "    \"\"\"\n",
    "    X = np.load(country_path + 'cluster_conv_features.npy')\n",
    "    X_nl = np.load(country_path + 'cluster_nightlights.npy').reshape(-1, 1)\n",
    "    y = np.load(country_path + 'cluster_assets.npy')\n",
    "    hhs = np.load(country_path + 'cluster_households.npy')\n",
    "    images = np.load(country_path + 'cluster_image_counts.npy')\n",
    "    # Filter out single households and <10 images\n",
    "    mask = np.logical_and((hhs >= 2), (images >= 10))\n",
    "    X = X[mask]\n",
    "    X_nl = X_nl[mask]\n",
    "    y = y[mask]\n",
    "    return X, X_nl, y\n",
    "\n",
    "\n",
    "def reduce_dimension(X, dimension):\n",
    "    \"\"\"\n",
    "    Uses PCA to reduce dimensionality of features.\n",
    "    \"\"\"\n",
    "    if dimension is not None:\n",
    "        pca = PCA(n_components=dimension)\n",
    "        X = pca.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def load_and_reduce_country(country_path, survey, dimension):\n",
    "    \"\"\"\n",
    "    Loads data for one country and reduces the dimensionality of features.\n",
    "    \"\"\"\n",
    "    if survey == 'lsms':\n",
    "        X, _, y = load_country_lsms(country_path)\n",
    "    elif survey == 'dhs':\n",
    "        X, _, y = load_country_dhs(country_path)\n",
    "    X = reduce_dimension(X, dimension)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def run_cv(X, y, k, k_inner, points, alpha_low, alpha_high, randomize=False):\n",
    "    \"\"\"\n",
    "    Runs nested cross-validation to make predictions and compute r-squared.\n",
    "    \"\"\"\n",
    "    alphas = np.logspace(alpha_low, alpha_high, points)\n",
    "    r2s = np.zeros((k,))\n",
    "    y_hat = np.zeros_like(y)\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    fold = 0\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        r2s, y_hat, fold = evaluate_fold(\n",
    "            X, y, train_idx, test_idx, k_inner, alphas, r2s, y_hat, fold,\n",
    "            randomize)\n",
    "    return y_hat, r2s.mean()\n",
    "\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Scales features using StandardScaler.\n",
    "    \"\"\"\n",
    "    X_scaler = StandardScaler(with_mean=True, with_std=False)\n",
    "    X_train = X_scaler.fit_transform(X_train)\n",
    "    X_test = X_scaler.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def train_and_predict_ridge(alpha, X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Trains ridge model and predicts test set.\n",
    "    \"\"\"\n",
    "    ridge = linear_model.Ridge(alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    y_hat = ridge.predict(X_test)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def predict_inner_test_fold(X, y, y_hat, train_idx, test_idx, alpha):\n",
    "    \"\"\"\n",
    "    Predicts inner test fold.\n",
    "    \"\"\"\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    X_train, X_test = scale_features(X_train, X_test)\n",
    "    y_hat[test_idx] = train_and_predict_ridge(alpha, X_train, y_train, X_test)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def find_best_alpha(X, y, k_inner, alphas):\n",
    "    \"\"\"\n",
    "    Finds the best alpha in an inner CV loop.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k_inner, shuffle=True)\n",
    "    best_alpha = 0\n",
    "    best_r2 = 0\n",
    "    for idx, alpha in enumerate(alphas):\n",
    "        y_hat = np.zeros_like(y)\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            y_hat = predict_inner_test_fold(\n",
    "                X, y, y_hat, train_idx, test_idx, alpha)\n",
    "        r2 = stats.pearsonr(y, y_hat)[0] ** 2\n",
    "        if r2 > best_r2:\n",
    "            best_alpha = alpha\n",
    "            best_r2 = r2\n",
    "    print('best alpha', best_alpha)\n",
    "    return best_alpha\n",
    "\n",
    "\n",
    "def evaluate_fold(\n",
    "    X, y, train_idx, test_idx, k_inner, alphas, r2s, y_hat, fold,\n",
    "        randomize):\n",
    "    \"\"\"\n",
    "    Evaluates one fold of outer CV.\n",
    "    \"\"\"\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    if randomize:\n",
    "        random.shuffle(y_train)\n",
    "    best_alpha = find_best_alpha(X_train, y_train, k_inner, alphas)\n",
    "    X_train, X_test = scale_features(X_train, X_test)\n",
    "    y_test_hat = train_and_predict_ridge(best_alpha, X_train, y_train, X_test)\n",
    "    r2 = stats.pearsonr(y_test, y_test_hat)[0] ** 2\n",
    "    r2s[fold] = r2\n",
    "    y_hat[test_idx] = y_test_hat\n",
    "    return r2s, y_hat, fold + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "x_hat = ss.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha 599.4842503189409\n",
      "best alpha 1668.100537200059\n",
      "best alpha 599.4842503189409\n",
      "best alpha 599.4842503189409\n",
      "best alpha 1668.100537200059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4186669257724608"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, _, r2 = predict_consumption(x_hat, y_log)\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha 12915.496650148827\n",
      "best alpha 35938.13663804626\n",
      "best alpha 1668.100537200059\n",
      "best alpha 35938.13663804626\n",
      "best alpha 12915.496650148827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19830535894547854"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, _, r2 = predict_consumption(x_hat, y)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick and dirty hand-testing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(0.7*(len(x_hat))) # let's use 70% of the data for training\n",
    "inds = np.arange(len(x_hat))\n",
    "train_ind = np.random.choice(inds, n_train, replace=False)\n",
    "valid_ind = np.delete(inds, train_ind)\n",
    "\n",
    "train_x = x_hat[train_ind]\n",
    "valid_x = x_hat[valid_ind]\n",
    "\n",
    "train_y = y_log[train_ind]\n",
    "valid_y = y_log[valid_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1443667920211187, 1.146268020299851)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.mean(), valid_y.mean() # let's make sure we don't have vastly different distributions of train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.889361380899413"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = linear_model.Ridge()\n",
    "ridge.fit(train_x, train_y)\n",
    "ridge.score(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7633108776629753"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ??? what happened ???\n",
    "# well, ridge regression is heavily affected by alpha (regularization) value\n",
    "# Jean et. al. code finds a good alpha, which we do not do here\n",
    "# if we use theirs, our ridge score will go up\n",
    "ridge.score(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5335034884094135"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge2 = linear_model.Ridge(alpha=1000) # the best alphas printed suggest using a high alphsa\n",
    "ridge2.fit(train_x, train_y)\n",
    "ridge2.score(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35512950020455225"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is closer to the r^2 using the paper's functions, as seen above\n",
    "# they did other things like CV and identifying best alphas\n",
    "# this was just meant to show a quick and dirty way of testing\n",
    "ridge2.score(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
